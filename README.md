![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Project%20Status-Completed-brightgreen)

# ğŸ“° BBC News Classification and Analysis

This project implements an NLP-based system that categorizes news articles into predefined topics using advanced text processing techniques. Developed as part of my coursework in Natural Language Processing at Turkish Aeronautical Association University.

## ğŸ‘¨â€ğŸ“ Student Information
- **Name**: Muhammet Ali Yoldar
- **Student ID**: 200444035
- **Program**: Computer Engineering, 4th Year
- **University**: Turkish Aeronautical Association University

---

## ğŸ“‹ Project Overview

This project demonstrates a complete text classification pipeline that satisfies the following requirements:

1. **Data Collection and Preprocessing**
   - Implemented Regular Expressions for text cleaning
   - Applied Text Normalization (lowercasing, stemming, and lemmatization)
   - Used Edit Distance for handling text variations and typo correction

2. **Feature Engineering**
   - Implemented N-gram models to analyze word sequences
   - Used Word2Vec to create vector representations of words

3. **Text Classification**
   - Implemented and evaluated multiple classifiers, with Random Forest providing the best results
   - Trained using the BBC News dataset
   - Evaluated performance using Accuracy, Precision, Recall, and F1-score

---

## ğŸ“ Dataset

- **Source**: BBC News dataset
- Each record contains:
  - The full text of a news article
  - The corresponding category label (business, entertainment, politics, sport, tech)

---

## ğŸ› ï¸ Methodology

1. **Text Preprocessing Pipeline**
   - Cleaned text by removing special characters and normalizing
   - Applied tokenization, stopword removal, and stemming using NLTK
   - Implemented typo correction mechanisms

2. **Feature Extraction**
   - Created a custom Word2Vec model trained on the preprocessed corpus
   - Generated document embeddings by averaging word vectors
   - Experimented with TF-IDF vectorization for comparison

3. **Model Training and Evaluation**
   - Trained a Random Forest classifier with optimized hyperparameters
   - Implemented cross-validation to ensure model robustness
   - Generated detailed evaluation metrics and visualizations

---

## ğŸ“Š Results

- **Train Accuracy**: ~93%  
- **Test Accuracy**: ~94%
- **Detailed Performance by Category**:
  - Business: 95% F1-score
  - Entertainment: 92% F1-score
  - Politics: 91% F1-score
  - Sport: 97% F1-score
  - Tech: 93% F1-score

---

## ğŸš€ Interactive Demo

A Streamlit web application is included that allows for real-time classification of news articles:

```
streamlit run app.py
```

---

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/muhammetaliyoldar/NLP-BBC-News-Classification.git

# Install dependencies
pip install -r requirements.txt

# Run the demo application
streamlit run app.py
```

---

## ğŸ” Future Improvements

- Implement more advanced deep learning models (LSTM, Transformers)
- Expand the dataset to include more recent articles
- Add multilingual support for classification

---

## ğŸ”„ Dengesiz Veri Seti YÃ¶netimi

Veri seti kategorileri arasÄ±nda dengesizlik olduÄŸu durumlarda, sÄ±nÄ±flandÄ±rma modellerinin performansÄ±nÄ± artÄ±rmak iÃ§in **SMOTE** (Synthetic Minority Over-sampling Technique) kullanÄ±lmÄ±ÅŸtÄ±r. Bu teknik:

- AzÄ±nlÄ±k sÄ±nÄ±flarÄ± iÃ§in sentetik Ã¶rnekler oluÅŸturarak veri setini dengeler
- SÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±n tÃ¼m kategoriler iÃ§in daha iyi genelleme yapmasÄ±nÄ± saÄŸlar
- EÄŸitim verisindeki kategori daÄŸÄ±lÄ±mÄ±nÄ±n eÅŸit olmasÄ±nÄ± saÄŸlayarak sÄ±nÄ±flandÄ±rma performansÄ±nÄ± iyileÅŸtirir

SMOTE ile eÄŸitim ve model deÄŸerlendirmesi iÃ§in:

```bash
python train_with_smote.py
```

Bu betik:
1. Word2Vec Ã¶zelliklerini yÃ¼kler
2. SMOTE uygulayarak dengesiz kategorileri dengeler
3. Lojistik Regresyon ve Random Forest modellerini eÄŸitir
4. Model performansÄ±nÄ± detaylÄ± olarak raporlar
5. Confusion matrix ve sÄ±nÄ±f daÄŸÄ±lÄ±m grafiklerini oluÅŸturur
6. EÄŸitilen modelleri `models` klasÃ¶rÃ¼ne kaydeder

![SMOTE ile SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±](outputs/smote_class_distribution.png)

## ğŸ“š Okunabilirlik Analizi

Proje kapsamÄ±nda, haber makalelerinin okunabilirlik seviyelerini deÄŸerlendirmek iÃ§in iki farklÄ± metrik kullanÄ±lmÄ±ÅŸtÄ±r:

1. **Flesch Reading Ease Score** - Metinlerin okunabilirlik kolaylÄ±ÄŸÄ±nÄ± Ã¶lÃ§er. YÃ¼ksek skorlar daha kolay okunabilen metinleri, dÃ¼ÅŸÃ¼k skorlar ise daha karmaÅŸÄ±k metinleri gÃ¶sterir.

2. **Dale-Chall Readability Score** - Metinlerin zorluk seviyesini sÃ¶zcÃ¼k karmaÅŸÄ±klÄ±ÄŸÄ±na gÃ¶re deÄŸerlendirir. YÃ¼ksek skorlar daha zor metinleri ifade eder.

Bu Ã¶lÃ§Ã¼mler, `textstat` kÃ¼tÃ¼phanesi kullanÄ±larak hesaplanmÄ±ÅŸtÄ±r. Hesaplama iÅŸlemleri iÃ§in:

```bash
python calculate_readability_scores.py
```

SonuÃ§larÄ± gÃ¶rselleÅŸtirmek iÃ§in:

```bash
python plot_readability_scores.py
```

![Readability Scores](outputs/readability_scores.png)

Analiz sonuÃ§larÄ±na gÃ¶re, farklÄ± haber kategorilerinin farklÄ± okunabilirlik seviyelerine sahip olduÄŸu gÃ¶rÃ¼lmektedir. Bu, hedef kitlesine gÃ¶re haber dilinin deÄŸiÅŸtiÄŸini gÃ¶stermektedir.

*This project was completed as part of the Natural Language Processing course requirements at Turkish Aeronautical Association University, Fall 2023.*

## Troubleshooting

### Model Loading Issues

If you encounter issues with the models not being found when running the Streamlit app:

1. Make sure you've run the notebook `bbc_news_classification_with_word2vec.ipynb` completely to generate the models
2. Verify that the following files exist:
   - `models/news_classifier_model.pkl` or `news_classifier_model.pkl` (in the root directory)
   - `models/word2vec.model` or `word2vec.model` (in the root directory)
3. If the models don't exist, you can run these sections of the notebook:
   - Section "Train Word2Vec Model" to create the word2vec.model
   - Section "Train and Evaluate Model" to create the news_classifier_model.pkl

The app is designed to look for models in both the `models/` directory and the root directory.

### NLTK Resource Issues

If you encounter issues with NLTK resources not being found (like 'punkt_tab' or other resources), you can run the provided script to download all required NLTK resources:

```bash
python download_nltk_resources.py
```

This will download all required NLTK packages to your user's NLTK data directory. If you still encounter issues, try adding the missing resources manually:

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```
